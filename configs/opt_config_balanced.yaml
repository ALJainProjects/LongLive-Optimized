# LongLive Optimization Configuration - Balanced Preset
#
# This is the recommended configuration for most use cases.
# Provides ~25-35% latency reduction with negligible quality impact.
#
# Usage:
#   python inference.py --optimized --opt-config configs/opt_config_balanced.yaml
#
# For custom configurations, copy this file and adjust parameters.

# Master switch
enabled: true

# CUDA Graphs - Captures denoising loop to eliminate kernel launch overhead
# Expected gain: 30-50% latency reduction
use_cuda_graphs: true
cuda_graph_warmup_steps: 3
cuda_graph_pool_size: 4  # Number of graph variants for different shapes

# KV Cache Configuration
# Static KV uses pre-allocated ring buffer (CUDA graph compatible)
# Quantized KV uses INT8/FP8 for memory bandwidth reduction
use_static_kv: true
use_quantized_kv: false  # Set to true for speed preset
kv_quantization: "int8"  # "int8" or "fp8"
local_attn_size: 12      # Frames in local attention window
sink_size: 3             # Frame-sink anchor frames

# VAE Configuration
# Async VAE overlaps decode with next frame generation
use_async_vae: true
vae_double_buffer: true  # Enable double buffering

# Prompt Cache
# Eliminates redundant text encoding for repeated/switched prompts
use_prompt_cache: true
prompt_cache_size: 100   # Max cached prompts (LRU eviction)

# Memory Configuration
# Pre-allocates all tensors to eliminate runtime allocation
use_memory_pool: true
use_pinned_memory: true  # Faster D2H transfers for output
preallocate_output: true

# Precision
# bfloat16 is recommended for H100
# fp8 provides additional speedup with small quality impact
model_dtype: "bfloat16"  # "bfloat16", "float16", "fp8"

# torch.compile (Alternative to CUDA graphs - do not use both)
use_torch_compile: false
compile_mode: "reduce-overhead"
compile_fullgraph: true

# Profiling
# Enable for latency analysis, disable for production
enable_profiling: false
profile_cuda_events: true
profile_memory: true

# Generation Defaults
num_frames: 120
num_frame_per_block: 3
denoising_steps: [1000, 750, 500, 250]

# Model Architecture (must match your model)
num_transformer_blocks: 30
frame_seq_length: 1560
num_heads: 12
head_dim: 128
